{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent Evaluation Dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial we will show you how to create Agent Evaluation Dataset with LangGraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reuse the LangGraph Agents for ETL error resolution in 04-sagemaker-mlflow-agents-introduction\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "## Import Data \n",
    "from data.data import log_data_set, log_data\n",
    "from data.solution_book import solution_book\n",
    "\n",
    "## Import MLflow Libs\n",
    "import sagemaker_mlflow \n",
    "import mlflow\n",
    "\n",
    "## Check AWS Credentials \n",
    "try:\n",
    "    boto3.client('bedrock-runtime')\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring AWS credentials: {e}\")\n",
    "    print(\"Please set your AWS credentials before proceeding.\")\n",
    "\n",
    "## Set up MLflow \n",
    "tracking_server_arn = \"ENTER YOUR MLFLOW TRACKING SERVER ARN HERE\" \n",
    "experiment_name = \"agent-mlflow-demo\"\n",
    "mlflow.set_tracking_uri(tracking_server_arn) \n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "## Set up LangChain Autolog \n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "## Import LangGraph Packages\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "## Define LangGraph Tools \n",
    "@tool \n",
    "def log_identifier(ticket_id: str) -> str:\n",
    "    \"\"\"Get error type from ticket number\n",
    "\n",
    "    Args:\n",
    "        ticket_id: ticket id\n",
    "\n",
    "    Returns:\n",
    "        an error type\n",
    "\n",
    "    \"\"\"\n",
    "    if ticket_id not in log_data_set:\n",
    "        return \"ticket id not found in the database\"\n",
    "    \n",
    "    for item in log_data:\n",
    "        if item[\"id\"] == ticket_id:\n",
    "            return item['error_name']\n",
    "\n",
    "@tool(return_direct=True)\n",
    "def information_retriever(error_type: str) -> str:\n",
    "    \"\"\"Retriever error solution based on error type\n",
    "\n",
    "    Args:\n",
    "        error_type: user input error type\n",
    "    \n",
    "    Returns:\n",
    "        a str of steps \n",
    "    \"\"\"\n",
    "\n",
    "    if error_type not in solution_book.keys():\n",
    "        return \"error type not found in the knowledge base, please use your own knowledge\"\n",
    "    \n",
    "    return solution_book[error_type]\n",
    "\n",
    "## Init LLM \n",
    "llm = init_chat_model(\n",
    "    model= \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    model_provider=\"bedrock_converse\",\n",
    ")\n",
    "\n",
    "## Build System Prompt \n",
    "system_prompt = \"\"\"\n",
    "You are an expert a resolving ETL errors. You are equiped with two tools: \n",
    "1. log_identifier: Get error type from ticket number\n",
    "2. information_retriever: Retriever error solution based on error type\n",
    "\n",
    "You will use the ticket ID to gather information about the error using the log_identifier tool. \n",
    "Then you should search the database for information on how to resolve the error using the information_retriever tool\n",
    "\n",
    "Return ONLY the numbered steps without any introduction or conclusion. Format as:\n",
    "1. step 1 text\n",
    "2. step 2 text\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "## Create ReAct Agent \n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools= [log_identifier, information_retriever], \n",
    "    prompt=system_prompt\n",
    ")\n",
    "\n",
    "def get_langGraph_agent_response(user_prompt):\n",
    "    # Prepare input for the agent\n",
    "    agent_input = {\"messages\": [{\"role\": \"user\", \"content\": user_prompt}]}\n",
    "    response = agent.invoke(agent_input)\n",
    "    return response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets introduce the test cases we have created, we have created four test cases, test_case_1, test_case_2, test_case_3 and test_case_4. Below we showed two examples for test_case_1 and test_case_2\n",
    "\n",
    "Each test case is a dictionary contains separates feilds: \n",
    "- ticket_id: ticket id (this is the input to the agent)\n",
    "- error_name: this is expected error name based on the ticekt id \n",
    "- solution: this is expected solution steps \n",
    "- expected_tools: expected tools to use, store in python list and order matters\n",
    "- expected_arguments: expected arguements for the above tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.test_cases_mlflow import TEST_CASES\n",
    "\n",
    "# Display first test case\n",
    "TEST_CASES[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will invoke agent for each of the test cases, we will store them into a dataframe, this dataframe will be used in evaluation. The dataframe will contains: \n",
    "1. inputs: this is the user input\n",
    "2. actual_output: Agent generated output \n",
    "3. expected_output: ground truth output \n",
    "\n",
    "### Create Evaluation Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "result = []\n",
    "\n",
    "for test_case in tqdm(TEST_CASES, desc=\"Processing test cases\"):\n",
    "    user_input = test_case['user_prompt']\n",
    "    error_solution = \"\\n\".join(test_case['solution'])\n",
    "    agent_response = get_langGraph_agent_response(user_input)\n",
    "\n",
    "    result.append({\n",
    "        'inputs': user_input,\n",
    "        'actual_output': agent_response,\n",
    "        'expected_output': error_solution\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store eval dataset as json\n",
    "eval_df.to_json(\"data/agent_evaluation_dataset.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
